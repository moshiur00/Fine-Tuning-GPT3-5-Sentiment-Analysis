{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "001749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Set API Key\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17884095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderlands murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming borders kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderlands kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderlands murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderlands murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0  Positive  im getting on borderlands and i will murder yo...   \n",
       "1  Positive  I am coming to the borders and I will kill you...   \n",
       "2  Positive  im getting on borderlands and i will kill you ...   \n",
       "3  Positive  im coming on borderlands and i will murder you...   \n",
       "4  Positive  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                    cleaned_text  \n",
       "0  im getting borderlands murder  \n",
       "1            coming borders kill  \n",
       "2    im getting borderlands kill  \n",
       "3   im coming borderlands murder  \n",
       "4  im getting borderlands murder  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Processed\n",
    "df = pd.read_csv(\"../data/processed/cleaned_tweets.csv\")\n",
    "df_validation = pd.read_csv(\"../data/processed/cleaned_tweets_validation.csv\")\n",
    "df.head()\n",
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f1b9c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34877</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>He said told u I'm getting in that box of a br...</td>\n",
       "      <td>said told u im getting box brain dead controll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21704</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Yo this looks LIT! CS: GO / Overwatch combo</td>\n",
       "      <td>yo looks lit cs go overwatch combo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47008</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@HomeDepot attention executive administrators....</td>\n",
       "      <td>attention executive administrators ever stores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Guy has notified me and says that my name has ...</td>\n",
       "      <td>guy notified says name forwarded litter list l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Positive</td>\n",
       "      <td>F Loving the new DLC!!!. RhandlerR RhandlerR R...</td>\n",
       "      <td>f loving new dlc rhandlerr rhandlerr rhandlerr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text  \\\n",
       "34877  Irrelevant  He said told u I'm getting in that box of a br...   \n",
       "21704    Positive        Yo this looks LIT! CS: GO / Overwatch combo   \n",
       "47008    Negative  @HomeDepot attention executive administrators....   \n",
       "7969   Irrelevant  Guy has notified me and says that my name has ...   \n",
       "454      Positive  F Loving the new DLC!!!. RhandlerR RhandlerR R...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "34877  said told u im getting box brain dead controll...  \n",
       "21704                 yo looks lit cs go overwatch combo  \n",
       "47008  attention executive administrators ever stores...  \n",
       "7969   guy notified says name forwarded litter list l...  \n",
       "454    f loving new dlc rhandlerr rhandlerr rhandlerr...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take at most 500 random samples\n",
    "df_sampled = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "df_validation_sampled = df_validation.sample(n=min(1000, len(df_validation)), random_state=42)\n",
    "\n",
    "# Display the samples\n",
    "df_sampled.head()\n",
    "df_validation_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1897139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for OpenAI Fine-Tuning (JSONL format)\n",
    "def create_jsonl(df, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['cleaned_text']) or pd.isna(row['sentiment']):\n",
    "                continue  # Skip rows with missing data\n",
    "            json_record = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You classify tweet sentiment as positive, negative, or neutral.\"},\n",
    "                    {\"role\": \"user\", \"content\": row['cleaned_text']},\n",
    "                    {\"role\": \"assistant\", \"content\": row['sentiment']}\n",
    "                ]\n",
    "            }\n",
    "            file.write(json.dumps(json_record) + \"\\n\")\n",
    "\n",
    "create_jsonl(df_sampled, \"../data/processed/train_data.jsonl\")\n",
    "create_jsonl(df_validation_sampled, \"../data/processed/test_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a3bd74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 959\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You classify tweet sentiment as positive, negative, or neutral.'}\n",
      "{'role': 'user', 'content': 'said told u im getting box brain dead controller player'}\n",
      "{'role': 'assistant', 'content': 'Irrelevant'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/processed/train_data.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d6f606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea63ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 29, 192\n",
      "mean / median: 42.50990615224192, 41.0\n",
      "p5 / p95: 31.0, 56.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 2\n",
      "mean / median: 1.1689259645464025, 1.0\n",
      "p5 / p95: 1.0, 2.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Token Count Check\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b83f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~40767 tokens that will be charged for during training\n",
      "By default, you'll train for 2 epochs on this dataset\n",
      "By default, you'll be charged for ~81534 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 2\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a16258c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "# Upload the file\n",
    "file_training = client.files.create(\n",
    "    file=open(\"../data/processed/train_data.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "file_validation = client.files.create(\n",
    "    file=open(\"../data/processed/test_data.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d97768ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fine-tuning\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=file_training.id,\n",
    "    validation_file = file_validation.id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d92d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
